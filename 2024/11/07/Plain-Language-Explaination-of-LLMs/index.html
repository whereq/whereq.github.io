<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.whereq.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Introduction Paper Introduction LLM in Brief Token Example Attempt Predicting the Next Token   Model Training Context Window Improving the Context Window From Markov Chains to Neural Networks   Trans">
<meta property="og:type" content="article">
<meta property="og:title" content="Plain Language Explanation of Large Models">
<meta property="og:url" content="https://www.whereq.com/2024/11/07/Plain-Language-Explaination-of-LLMs/index.html">
<meta property="og:site_name" content="WhereQ">
<meta property="og:description" content="Introduction Paper Introduction LLM in Brief Token Example Attempt Predicting the Next Token   Model Training Context Window Improving the Context Window From Markov Chains to Neural Networks   Trans">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_01.jpg">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_02.png">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_03.png">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_04.png">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_05.jpg">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_06.jpg">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_07.png">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_08.png">
<meta property="og:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_09.png">
<meta property="article:published_time" content="2024-11-07T16:58:04.000Z">
<meta property="article:modified_time" content="2025-11-21T21:23:35.866Z">
<meta property="article:author" content="Dazhi Zhang">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.whereq.com/images/Plain-Language-Explaination-of-LLMs/Image_01.jpg">


<link rel="canonical" href="https://www.whereq.com/2024/11/07/Plain-Language-Explaination-of-LLMs/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.whereq.com/2024/11/07/Plain-Language-Explaination-of-LLMs/","path":"2024/11/07/Plain-Language-Explaination-of-LLMs/","title":"Plain Language Explanation of Large Models"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Plain Language Explanation of Large Models | WhereQ</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">WhereQ</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-articles"><a href="/articles/" rel="section"><i class="fa fa-file fa-fw"></i>Articles</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper-Introduction"><span class="nav-number">2.</span> <span class="nav-text">Paper Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM-in-Brief"><span class="nav-number">3.</span> <span class="nav-text">LLM in Brief</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Token"><span class="nav-number">4.</span> <span class="nav-text">Token</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Example"><span class="nav-number">4.1.</span> <span class="nav-text">Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attempt"><span class="nav-number">4.2.</span> <span class="nav-text">Attempt</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Predicting-the-Next-Token"><span class="nav-number">4.3.</span> <span class="nav-text">Predicting the Next Token</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Training"><span class="nav-number">5.</span> <span class="nav-text">Model Training</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Context-Window"><span class="nav-number">6.</span> <span class="nav-text">Context Window</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improving-the-Context-Window"><span class="nav-number">7.</span> <span class="nav-text">Improving the Context Window</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#From-Markov-Chains-to-Neural-Networks"><span class="nav-number">7.1.</span> <span class="nav-text">From Markov Chains to Neural Networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Transformer-and-Attention-Mechanism"><span class="nav-number">8.</span> <span class="nav-text">Transformer and Attention Mechanism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Self-Attention-Self-Attention-Mechanism"><span class="nav-number">9.</span> <span class="nav-text">Self-Attention (Self-Attention Mechanism)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-Attention-Introduction"><span class="nav-number">9.1.</span> <span class="nav-text">Self-Attention Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Workflow"><span class="nav-number">9.2.</span> <span class="nav-text">Workflow</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Formula"><span class="nav-number">9.3.</span> <span class="nav-text">Formula</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Head-Attention"><span class="nav-number">10.</span> <span class="nav-text">Multi-Head Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">10.1.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Add-Norm-and-Feed-Forward"><span class="nav-number">11.</span> <span class="nav-text">Add &amp; Norm and Feed Forward</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Add-Norm-Residual-Connection-and-Layer-Normalization"><span class="nav-number">11.1.</span> <span class="nav-text">Add &amp; Norm (Residual Connection and Layer Normalization)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Residual-Connection"><span class="nav-number">11.1.1.</span> <span class="nav-text">Residual Connection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Layer-Normalization"><span class="nav-number">11.1.2.</span> <span class="nav-text">Layer Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Combined-Use"><span class="nav-number">11.1.3.</span> <span class="nav-text">Combined Use</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feed-Forward-Neural-Network"><span class="nav-number">11.2.</span> <span class="nav-text">Feed Forward Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Purpose"><span class="nav-number">11.2.1.</span> <span class="nav-text">Purpose</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Structure"><span class="nav-number">11.2.2.</span> <span class="nav-text">Structure</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary-1"><span class="nav-number">11.3.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decoder-Structure"><span class="nav-number">12.</span> <span class="nav-text">Decoder Structure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Appendix"><span class="nav-number">13.</span> <span class="nav-text">Appendix</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Dazhi Zhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">148</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">132</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">87</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.whereq.com/2024/11/07/Plain-Language-Explaination-of-LLMs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Dazhi Zhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WhereQ">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Plain Language Explanation of Large Models | WhereQ">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Plain Language Explanation of Large Models
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-11-07 11:58:04" itemprop="dateCreated datePublished" datetime="2024-11-07T11:58:04-05:00">2024-11-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-11-21 16:23:35" itemprop="dateModified" datetime="2025-11-21T16:23:35-05:00">2025-11-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#paper-introduction">Paper Introduction</a></li>
<li><a href="#llm-in-brief">LLM in Brief</a></li>
<li><a href="#token">Token</a><ul>
<li><a href="#example">Example</a></li>
<li><a href="#attempt">Attempt</a></li>
<li><a href="#predicting-the-next-token">Predicting the Next Token</a></li>
</ul>
</li>
<li><a href="#model-training">Model Training</a></li>
<li><a href="#context-window">Context Window</a></li>
<li><a href="#improving-the-context-window">Improving the Context Window</a><ul>
<li><a href="#from-markov-chains-to-neural-networks">From Markov Chains to Neural Networks</a></li>
</ul>
</li>
<li><a href="#transformer-and-attention-mechanism">Transformer and Attention Mechanism</a></li>
<li><a href="#self-attention-self-attention-mechanism">Self-Attention (Self-Attention Mechanism)</a><ul>
<li><a href="#self-attention-introduction">Self-Attention Introduction</a></li>
<li><a href="#workflow">Workflow</a></li>
<li><a href="#formula">Formula</a></li>
</ul>
</li>
<li><a href="#multi-head-attention">Multi-Head Attention</a><ul>
<li><a href="#summary">Summary</a></li>
</ul>
</li>
<li><a href="#add--norm-and-feed-forward">Add &amp; Norm and Feed Forward</a><ul>
<li><a href="#add--norm-residual-connection-and-layer-normalization">Add &amp; Norm (Residual Connection and Layer Normalization)</a><ul>
<li><a href="#residual-connection">Residual Connection</a></li>
<li><a href="#layer-normalization">Layer Normalization</a></li>
<li><a href="#combined-use">Combined Use</a></li>
</ul>
</li>
<li><a href="#feed-forward-neural-network">Feed Forward Neural Network</a><ul>
<li><a href="#purpose">Purpose</a></li>
<li><a href="#structure">Structure</a></li>
</ul>
</li>
<li><a href="#summary-1">Summary</a></li>
</ul>
</li>
<li><a href="#decoder-structure">Decoder Structure</a></li>
<li><a href="#appendix">Appendix</a></li>
</ul>
<hr>
<p><a name="introduction"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>This document aims to provide a detailed explanation of the current mainstream large model architectures, such as the Transformer architecture. We will cover various aspects from a technical overview, architecture introduction to specific model implementations. Through this document, we hope to provide readers with a comprehensive understanding, helping them grasp the working principles of large models and enhance their technical foundation for communicating with clients. This document is suitable for individuals interested in large models.</p>
<p><a name="paper-introduction"></a></p>
<h2 id="Paper-Introduction"><a href="#Paper-Introduction" class="headerlink" title="Paper Introduction"></a>Paper Introduction</h2><p><strong>Paper Title:</strong> <em>Attention is all you need</em></p>
<p><strong>Release Date:</strong> 2017&#x2F;06&#x2F;12</p>
<p><strong>Published By:</strong> Google, University of Toronto</p>
<p><strong>Brief Summary:</strong> The progenitor of all LLMs, the foundational architecture for a new era in NLP.</p>
<p><strong>Chinese Summary:</strong> Traditional sequence-to-sequence models use complex recurrent or convolutional neural networks, including encoders and decoders. The best-performing models connect the encoder and decoder through attention mechanisms.</p>
<p>The author’s team proposes a new simple network structure, Transformer, which is entirely based on attention mechanisms and no longer uses recurrence and convolution.</p>
<p>Experiments on two machine translation tasks show that these models perform superiorly in quality and are easier to parallelize, significantly reducing the training time required.</p>
<p>The model achieved 28.4 BLEU on the WMT 2014 English-German translation task, over 2 BLEU higher than the best existing results (including ensemble models). On the WMT 2014 English-French translation task, the model achieved a new single-model best BLEU score of 41.8 after training on eight GPUs for 3.5 days, with training costs only a fraction of the best models in the literature.</p>
<p>The Transformer demonstrates generalization capabilities across other tasks, whether with extensive or limited training data, successfully applied to English constituency parsing.</p>
<p><strong>Paper Link:</strong> <a href="pdf/Plain-Language-Explaination-of-LLMs/Attention_Is_All_You_Need.pdf">Attention Is All You Need</a></p>
<p><strong>Core Technology:</strong> Model Architecture (here, leave a general impression of encode + decode)</p>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_01.jpg" alt="Model Architecture"></p>
<p><a name="llm-in-brief"></a></p>
<h2 id="LLM-in-Brief"><a href="#LLM-in-Brief" class="headerlink" title="LLM in Brief"></a>LLM in Brief</h2><p>Many believe that large models can directly answer questions or engage in conversations. However, their core function is to predict the next likely word, or “Token,” based on the input text. This predictive ability makes LLMs excel in various applications, including but not limited to:</p>
<ul>
<li><strong>Text Generation:</strong> LLMs can generate coherent and meaningful text passages for writing assistance, content creation, etc.</li>
<li><strong>Question Answering Systems:</strong> By understanding the context of a question, LLMs can generate accurate answers, widely used in intelligent customer service and information retrieval.</li>
<li><strong>Translation:</strong> LLMs can perform high-quality language translation based on context, supporting multilingual communication.</li>
<li><strong>Text Summarization:</strong> LLMs can extract key content from long documents to generate concise summaries, facilitating quick understanding.</li>
<li><strong>Dialogue Systems:</strong> LLMs can simulate human conversations, providing natural and smooth interaction experiences, applied in chatbots and virtual assistants.</li>
</ul>
<p>By understanding the concept of Tokens, we can better grasp the working principles of LLMs and their powerful capabilities in practical applications.</p>
<p><a name="token"></a></p>
<h2 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h2><p><a name="token-example"></a></p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>When discussing Tokens, it’s impossible not to mention a recent low-level error in a large model: “How many ‘r’s are in ‘Strawberry’?”</p>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_02.png" alt="Strawberry"></p>
<p>After the ridicule, people calmed down and began to think: what is the essence behind this low-level error?</p>
<p>It is generally believed that it is the fault of Tokenization.</p>
<p>In China, Tokenization is often translated as “word segmentation.” This translation is somewhat misleading because the Tokens in Tokenization do not necessarily refer to words; they can also be punctuation marks, numbers, or parts of a word. For example, in a tool provided by OpenAI, the word “Strawberry” is divided into three Tokens: Str-aw-berry. In this case, asking the AI large model to count how many ‘r’s are in the word is indeed a challenge for it.</p>
<p>To let everyone intuitively see the world of text in the eyes of large models, Karpathy specially wrote a small program to represent Tokens with emojis.</p>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_03.png" alt="Tokens With Emojis"></p>
<p><a name="token-attempt"></a></p>
<h3 id="Attempt"><a href="#Attempt" class="headerlink" title="Attempt"></a>Attempt</h3><p>A Token is the basic unit of text that LLMs understand. Although it is convenient to consider a Token as a word, for LLMs, the goal is to encode text as efficiently as possible. Therefore, in many cases, a Token represents a character sequence shorter or longer than an entire word. Punctuation marks and spaces are also represented as Tokens, either individually or combined with other characters. Each Token in the LLM vocabulary has a unique identifier, usually a number. LLMs use tokenizers to convert regular text strings into equivalent lists of Token numbers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the encoder for the GPT-2 model</span></span><br><span class="line">encoding = tiktoken.encoding_for_model(<span class="string">&quot;gpt-2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode example sentence</span></span><br><span class="line">encoded_text = encoding.encode(<span class="string">&quot;A journey of a thousand miles begins with a single step.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoded_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decode back to original text</span></span><br><span class="line">decoded_text = encoding.decode(encoded_text)</span><br><span class="line"><span class="built_in">print</span>(decoded_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode and decode individual tokens</span></span><br><span class="line"><span class="built_in">print</span>(encoding.decode([<span class="number">32</span>]))  <span class="comment"># &#x27;A&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(encoding.decode([<span class="number">7002</span>])) <span class="comment"># &#x27; journey&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(encoding.decode([<span class="number">286</span>]))   <span class="comment"># &#x27;of&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode the word &quot;thousand&quot;</span></span><br><span class="line">thousand_encoded = encoding.encode(<span class="string">&quot;thousand&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(thousand_encoded)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decode the encoded result of &quot;thousand&quot;</span></span><br><span class="line"><span class="built_in">print</span>(encoding.decode([<span class="number">400</span>])) <span class="comment"># &#x27;th&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(encoding.decode([<span class="number">29910</span>]))   <span class="comment"># &#x27;ousand&#x27;</span></span><br><span class="line"></span><br><span class="line">[<span class="number">32</span>, <span class="number">7002</span>, <span class="number">286</span>, <span class="number">257</span>, <span class="number">7319</span>, <span class="number">4608</span>, <span class="number">6140</span>, <span class="number">351</span>, <span class="number">257</span>, <span class="number">2060</span>, <span class="number">2239</span>, <span class="number">13</span>]</span><br><span class="line">A journey of a thousand miles begins <span class="keyword">with</span> a single step.</span><br><span class="line">A</span><br><span class="line"> journey</span><br><span class="line"> of</span><br><span class="line">[<span class="number">400</span>, <span class="number">29910</span>]</span><br><span class="line">th</span><br><span class="line">ousand</span><br></pre></td></tr></table></figure>

<p><a name="predicting-the-next-token"></a></p>
<h3 id="Predicting-the-Next-Token"><a href="#Predicting-the-Next-Token" class="headerlink" title="Predicting the Next Token"></a>Predicting the Next Token</h3><p>As mentioned above, given a piece of text, the task of a language model is to predict the next Token. If expressed in Python pseudocode, it looks like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = predict_next_token([<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;journey&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;a&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>Here, the <code>predict_next_token</code> function receives a series of input Tokens converted from a user-provided prompt. In this example, we assume each word constitutes a separate Token. In reality, each Token is encoded as a number rather than being directly passed into the model as text. The output of the function is a data structure containing the probability values for each possible Token in the vocabulary to appear after the current input sequence.</p>
<p>The language model needs to learn to make such predictions through a training process. During training, the model is exposed to a large amount of text data, learning language patterns and rules. After training, the model can use the learned knowledge to estimate the probability of the next Token for any given Token sequence.</p>
<p>To generate continuous text, the model needs to repeatedly call itself, generating a new Token each time and adding it to the existing sequence until the preset length is reached. Below is a more detailed Python pseudocode showing this process:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">prompt, num_tokens, hyperparameters</span>):</span><br><span class="line">    tokens = tokenize(prompt)  <span class="comment"># Convert prompt to a list of Tokens</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_tokens):  <span class="comment"># Repeat for the specified number of Tokens</span></span><br><span class="line">        predictions = predict_next_token(tokens)  <span class="comment"># Get predictions for the next Token</span></span><br><span class="line">        next_token = select_next_token(predictions, hyperparameters)  <span class="comment"># Select the next Token based on probabilities</span></span><br><span class="line">        tokens.append(next_token)  <span class="comment"># Add the selected Token to the list</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(tokens)  <span class="comment"># Merge the Token list into a string</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Helper function to select the next Token</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">select_next_token</span>(<span class="params">predictions, hyperparameters</span>):</span><br><span class="line">    <span class="comment"># Adjust Token selection using strategies like temperature, top-k, or top-p</span></span><br><span class="line">    <span class="comment"># Implementation details depend on the specific hyperparameter settings</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>In this code, the <code>generate_text</code> function accepts a prompt string, the number of Tokens to generate, and a set of hyperparameters as input. The <code>tokenize</code> function is responsible for converting the prompt into a list of Tokens, while the <code>select_next_token</code> function selects the next Token based on the predicted probability distribution. By adjusting the hyperparameters in <code>select_next_token</code>, such as temperature, top-k, and top-p, the diversity and creativity of the generated text can be controlled. As the loop iterates, new Tokens are continuously added to the sequence, eventually forming coherent text output.</p>
<p><a name="model-training"></a></p>
<h2 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h2><p>Imagine we are training a model to predict the next word in a sentence. This is like playing a word-guessing game where the model needs to guess the next word based on the words already present. For a simplified vocabulary, let’s assume there are only five words:</p>
<p>[‘I’, ‘you’, ‘love’, ‘oranges’, ‘grapes’]</p>
<p>We do not consider spaces and punctuation, focusing only on these words.</p>
<p>We have three sentences as training data:</p>
<ul>
<li>I love oranges</li>
<li>I love grapes</li>
<li>you love oranges</li>
</ul>
<p>We can imagine a 5x5 table where each cell represents the number of times one word follows another. This table might look like this:</p>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_04.png" alt="Cell Matrix"></p>
<p>In this example, “I” is followed by “love” twice, and “love” is followed by “oranges” once and “grapes” twice. To make this table useful, we need to convert these counts into probabilities so that the model can predict the likelihood of the next word. For example, the probability of “oranges” following “love” is 1&#x2F;3 (about 33.3%), and the probability of “grapes” following “love” is 2&#x2F;3 (about 66.7%).</p>
<p>However, we encounter a problem: “oranges” and “grapes” do not appear after other words. This means that without additional information, the model will not be able to predict what might follow these words. To solve this problem, we can assume that any word in the vocabulary might follow any other word, although this is not perfect, it ensures that the model can make predictions even with limited training data.</p>
<p>In the real world, large language models (LLMs) use vast amounts of training data, reducing the chances of such “gaps” occurring. However, due to the infrequent occurrence of certain word combinations in the training data, LLMs may perform poorly in some cases, resulting in generated text that is grammatically correct but logically flawed or inconsistent. This phenomenon is sometimes referred to as “model hallucination.”</p>
<p><a name="context-window"></a></p>
<h2 id="Context-Window"><a href="#Context-Window" class="headerlink" title="Context Window"></a>Context Window</h2><p>In previous discussions, we mentioned using Markov chains to train small models, which rely on only the last Token to predict the next Token. This means that any text before the last Token does not affect the prediction, so the context window is very small, only one Token. Due to the small context window, the model easily “forgets” previous information, leading to generated text that lacks consistency, jumping from one word to another.</p>
<p><a name="improving-the-context-window"></a></p>
<h2 id="Improving-the-Context-Window"><a href="#Improving-the-Context-Window" class="headerlink" title="Improving the Context Window"></a>Improving the Context Window</h2><p>To improve the model’s prediction quality, we can try to increase the size of the context window. For example, if we use a two-Token context window, we need to build a larger probability matrix where each row represents all possible two-Token sequences. For a five-Token vocabulary, this will add 25 rows (5^2). Each time the <code>predict_next_token()</code> function is called, the last two Tokens of the input will be used to find the corresponding row in the probability table.</p>
<p>However, even with a two-Token context window, the generated text may still lack coherence. To generate more consistent and meaningful text, the context window size needs to be further increased. For example, increasing the context window to three Tokens will increase the number of rows in the probability table to 125 (5^3), but this is still insufficient to generate high-quality text.</p>
<p>As the context window increases, the size of the probability table grows exponentially. Taking the GPT-2 model as an example, it uses a 1024-Token context window. If we were to implement such a large context window using Markov chains as in the previous example, each row of the probability table would need to represent a sequence of length between 1 and 1024 Tokens. For a vocabulary of 5 Tokens, the number of possible sequences is 5^1024, an astronomical number. This number is so large that it is impractical to store and process such a massive probability table. Therefore, Markov chains face serious scalability issues when dealing with large context windows.</p>
<h3 id="From-Markov-Chains-to-Neural-Networks"><a href="#From-Markov-Chains-to-Neural-Networks" class="headerlink" title="From Markov Chains to Neural Networks"></a>From Markov Chains to Neural Networks</h3><p>Clearly, the probability table method is infeasible when dealing with large context windows. We need a more efficient method to predict the next Token. This is where neural networks come into play. Neural networks are special functions that accept input data, perform a series of calculations, and then output predictions. For language models, the input is a series of Tokens, and the output is the probability distribution of the next Token.</p>
<p>The key to neural networks lies in their parameters. These parameters are gradually adjusted during training to optimize the model’s prediction performance. The training process involves extensive mathematical operations, including forward propagation and backpropagation. Forward propagation refers to the input data passing through the network’s layers to generate predictions; backpropagation adjusts the network’s parameters based on the difference between the predictions and the true labels to reduce errors.</p>
<p>Modern language models, such as GPT-2, GPT-3, and GPT-4, use very deep neural networks with hundreds of millions or even trillions of parameters. The training process for these models is very complex and usually takes weeks or even months. Despite this, well-trained LLMs maintain high coherence and consistency in text generation, thanks to their powerful context understanding and generation capabilities.</p>
<p><a name="transformer-and-attention-mechanism"></a></p>
<h2 id="Transformer-and-Attention-Mechanism"><a href="#Transformer-and-Attention-Mechanism" class="headerlink" title="Transformer and Attention Mechanism"></a>Transformer and Attention Mechanism</h2><p>The Transformer is one of the most popular neural network architectures, particularly suited for natural language processing tasks. The core feature of the Transformer model is its attention mechanism. The attention mechanism allows the model to dynamically focus on different parts of the input sequence when processing it, thereby better capturing context information.</p>
<p>The attention mechanism was initially applied to machine translation tasks to help the model identify key information in the input sequence. Through the attention mechanism, the model can “focus” on important Tokens in the input sequence, thereby generating more accurate translation results. In language generation tasks, the attention mechanism plays an equally important role, enabling the model to consider multiple Tokens in the context window when generating the next Token, thereby generating more coherent and meaningful text.</p>
<p>In summary, although Markov chains provide a simple method for text generation, they have obvious limitations when dealing with large context windows. Modern language models overcome these limitations by using neural networks and attention mechanisms, achieving efficient and high-quality text generation.</p>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_05.jpg" alt="Framework-05"></p>
<p>Returning to the framework diagram, the input representation <strong>x</strong> of a word in the Transformer is obtained by adding the word <strong>Embedding</strong> and the position <strong>Embedding</strong> (Positional Encoding).</p>
<p>There are many ways to obtain the word Embedding, such as using pre-trained algorithms like Word2Vec, Glove, or training it in the Transformer.</p>
<p>In addition to the word Embedding, the Transformer also uses position Embedding to represent the position of a word in the sentence. <strong>Because the Transformer does not use an RNN structure but uses global information, it cannot utilize the order information of words, which is very important for NLP.</strong> Therefore, the Transformer uses position Embedding to preserve the relative or absolute position of words in the sequence.</p>
<p>In short: taking “apple” as an example</p>
<p>In “There are apples and bananas in the fruit store,” “apple” refers to the fruit, while in “The store has just launched the latest Apple 16,” “apple” represents the brand.</p>
<p><a name="self-attention"></a></p>
<h2 id="Self-Attention-Self-Attention-Mechanism"><a href="#Self-Attention-Self-Attention-Mechanism" class="headerlink" title="Self-Attention (Self-Attention Mechanism)"></a>Self-Attention (Self-Attention Mechanism)</h2><p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_06.jpg" alt="Self Attention"></p>
<p>The above image is the internal structure diagram of the Transformer in the paper, with the left side being the Encoder block and the right side being the Decoder block. The part circled in red is <strong>Multi-Head Attention</strong>, which consists of multiple <strong>Self-Attention</strong> layers. It can be seen that the Encoder block contains one Multi-Head Attention, while the Decoder block contains two Multi-Head Attention (one of which uses Masked). Above the Multi-Head Attention is an Add &amp; Norm layer, where Add represents a residual connection (Residual Connection) to prevent network degradation, and Norm represents Layer Normalization to normalize the activation values of each layer.</p>
<p>Since <strong>Self-Attention</strong> is the key point of the Transformer, we focus on Multi-Head Attention and Self-Attention. First, let’s take a detailed look at the internal logic of Self-Attention.</p>
<p>Of course, we can understand the Self-Attention mechanism in a more concise way.</p>
<p><a name="self-attention-introduction"></a></p>
<h3 id="Self-Attention-Introduction"><a href="#Self-Attention-Introduction" class="headerlink" title="Self-Attention Introduction"></a>Self-Attention Introduction</h3><p><strong>Self-Attention</strong> is a method that allows the model to focus on different parts of the sequence data when processing it, especially suitable for handling long texts. It achieves this by calculating the relevance between each element in the sequence and all other elements.</p>
<p><a name="self-attention-workflow"></a></p>
<h3 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h3><ol>
<li><p><strong>Transformation:</strong></p>
<ul>
<li>Each input element (such as a word) is transformed into three vectors: <strong>Query (query), Key (key), and Value (value)</strong>. These vectors are obtained by multiplying the input vector by three different weight matrices WQ, WK, and WV.</li>
</ul>
</li>
<li><p><strong>Calculate Attention Scores:</strong></p>
<ul>
<li>For each element, use its <strong>Query</strong> vector to perform dot product operations with the <strong>Key</strong> vectors of all other elements, obtaining a list of scores. This score represents the relevance of the current element to all other elements.</li>
</ul>
</li>
<li><p><strong>Normalization:</strong></p>
<ul>
<li>Normalize these scores through the <strong>softmax</strong> function to obtain a probability distribution, representing the attention weights of the current element to all other elements.</li>
</ul>
</li>
<li><p><strong>Weighted Sum:</strong></p>
<ul>
<li>Use these attention weights to perform a weighted sum of the <strong>Value</strong> vectors of all elements, obtaining the final output vector.</li>
</ul>
</li>
</ol>
<p><a name="self-attention-formula"></a></p>
<h3 id="Formula"><a href="#Formula" class="headerlink" title="Formula"></a>Formula</h3><p>Suppose the input sequence is X &#x3D; [x1, x2, …, xn], where each xi is a vector.</p>
<ol>
<li>Transformation:</li>
</ol>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_07.png" alt="Transformation"></p>
<ol start="2">
<li>Calculate Attention Scores:</li>
</ol>
<p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_08.png" alt="Calculate Attention"></p>
<p>   where dk is the dimension of the Key vector.</p>
<ol start="3">
<li><p>Normalization: Attention Weights &#x3D; softmax(Scores)</p>
</li>
<li><p>Weighted Sum: Output &#x3D; Attention Weights · V</p>
</li>
</ol>
<p><a name="multi-head-attention"></a></p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><ul>
<li><strong>Multi-Head Attention</strong> is designed to allow the model to capture information from multiple different perspectives. Specifically, multiple Self-Attention layers (each called a “head”) are run in parallel, and then all heads’ outputs are concatenated and passed through a linear transformation.</li>
</ul>
<p><a name="multi-head-attention-summary"></a></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li><strong>Self-Attention</strong> allows the model to focus on different parts of the sequence, thereby better capturing long-range dependencies.</li>
<li><strong>Multi-Head Attention</strong> enhances the model’s expressive power by using multiple Self-Attention layers, allowing it to consider information from multiple perspectives.</li>
</ul>
<p><a name="add-norm-and-feed-forward"></a></p>
<h2 id="Add-Norm-and-Feed-Forward"><a href="#Add-Norm-and-Feed-Forward" class="headerlink" title="Add &amp; Norm and Feed Forward"></a>Add &amp; Norm and Feed Forward</h2><p><a name="add-norm-residual-connection"></a></p>
<h3 id="Add-Norm-Residual-Connection-and-Layer-Normalization"><a href="#Add-Norm-Residual-Connection-and-Layer-Normalization" class="headerlink" title="Add &amp; Norm (Residual Connection and Layer Normalization)"></a>Add &amp; Norm (Residual Connection and Layer Normalization)</h3><h4 id="Residual-Connection"><a href="#Residual-Connection" class="headerlink" title="Residual Connection"></a>Residual Connection</h4><ul>
<li><strong>Purpose:</strong> The residual connection helps in training deep networks by allowing the gradient to flow directly through the network, mitigating the vanishing gradient problem.</li>
<li><strong>Operation:</strong> The residual connection adds the input of a layer directly to its output:<br>[<br>\text{Output} &#x3D; F(x) + x<br>]<br>where ( F(x) ) is the output of the layer.</li>
</ul>
<h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><ul>
<li><strong>Purpose:</strong> Layer normalization stabilizes the training process by normalizing the activations of each layer, ensuring that the mean and variance of the activations remain consistent.</li>
<li><strong>Operation:</strong> For each sample in the batch, layer normalization normalizes the activations across the feature dimension:<br>[<br>\text{LayerNorm}(x) &#x3D; \frac{x - \mu}{\sigma} \cdot \gamma + \beta<br>]<br>where ( \mu ) and ( \sigma ) are the mean and standard deviation of the activations, and ( \gamma ) and ( \beta ) are learnable parameters.</li>
</ul>
<h4 id="Combined-Use"><a href="#Combined-Use" class="headerlink" title="Combined Use"></a>Combined Use</h4><ul>
<li><p><strong>Steps:</strong></p>
<ol>
<li>First, calculate the output ( F(x) ) of a layer.</li>
<li>Add the input ( x ) to ( F(x) ), obtaining ( y &#x3D; F(x) + x ).</li>
<li>Perform layer normalization on ( y ) to obtain the final output.</li>
</ol>
</li>
</ul>
<p><a name="feed-forward-neural-network"></a></p>
<h3 id="Feed-Forward-Neural-Network"><a href="#Feed-Forward-Neural-Network" class="headerlink" title="Feed Forward Neural Network"></a>Feed Forward Neural Network</h3><h4 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h4><ul>
<li><strong>Add Nonlinearity:</strong> Makes the model more flexible, capable of handling more complex data.</li>
</ul>
<h4 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h4><ul>
<li><p><strong>Two-Layer Fully Connected Network:</strong></p>
<ol>
<li>The first layer: Transform the input through a linear transformation (multiply by a matrix), then process it with the ReLU activation function.</li>
<li>The second layer: Transform it again through a linear transformation (multiply by another matrix).</li>
</ol>
</li>
</ul>
<p><a name="feed-forward-summary"></a></p>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li><strong>Add &amp; Norm:</strong> Through residual connections and layer normalization, the model becomes more stable and trains faster.</li>
<li><strong>Feed Forward:</strong> Through a two-layer fully connected network, it adds flexibility to the model, enabling it to handle more complex data.</li>
</ul>
<p><a name="decoder-structure"></a></p>
<h2 id="Decoder-Structure"><a href="#Decoder-Structure" class="headerlink" title="Decoder Structure"></a>Decoder Structure</h2><p><img src="/images/Plain-Language-Explaination-of-LLMs/Image_09.png" alt="Decoder Structure"></p>
<p>The red part in the above image is the structure of the Transformer’s Decoder block, similar to the Encoder block but with some differences:</p>
<ul>
<li>Contains two Multi-Head Attention layers.</li>
<li>The first Multi-Head Attention layer uses a Masked operation.</li>
<li>The second Multi-Head Attention layer’s <strong>K, V</strong> matrices use the Encoder’s <strong>encoded information matrix C</strong> for calculation, while <strong>Q</strong> uses the output of the previous Decoder block for calculation.</li>
<li>Finally, a Softmax layer calculates the probability of the next translated word.</li>
</ul>
<p><a name="appendix"></a></p>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the positional encoding layer, used to add positional information to the input sequence</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># Initialize the positional encoding tensor</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        <span class="comment"># Calculate the sine and cosine values</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># Register the positional encoding as a buffer</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Add the positional encoding to the input</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the Transformer-based model class</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim, d_model=<span class="number">512</span>, nhead=<span class="number">8</span>, num_encoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model_type = <span class="string">&#x27;Transformer&#x27;</span></span><br><span class="line">        <span class="comment"># Define the embedding layer</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(input_dim, d_model)</span><br><span class="line">        <span class="comment"># Define the positional encoding layer</span></span><br><span class="line">        <span class="variable language_">self</span>.pos_encoder = PositionalEncoding(d_model)</span><br><span class="line">        <span class="comment"># Define the encoder layers</span></span><br><span class="line">        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="comment"># Define the decoder layer</span></span><br><span class="line">        <span class="variable language_">self</span>.decoder = nn.Linear(d_model, output_dim)</span><br><span class="line">        <span class="comment"># Initialize the weights</span></span><br><span class="line">        <span class="variable language_">self</span>.init_weights()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">self</span>):</span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        <span class="variable language_">self</span>.embedding.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        <span class="variable language_">self</span>.decoder.bias.data.zero_()</span><br><span class="line">        <span class="variable language_">self</span>.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="comment"># Embed the input and multiply by the square root of d_model</span></span><br><span class="line">        src = <span class="variable language_">self</span>.embedding(src) * math.sqrt(<span class="variable language_">self</span>.d_model)</span><br><span class="line">        src = <span class="variable language_">self</span>.pos_encoder(src)</span><br><span class="line">        <span class="comment"># Encode the input</span></span><br><span class="line">        output = <span class="variable language_">self</span>.transformer_encoder(src, src_mask)</span><br><span class="line">        <span class="comment"># Decode the output</span></span><br><span class="line">        output = <span class="variable language_">self</span>.decoder(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a triangular matrix as a mask</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_square_subsequent_mask</span>(<span class="params">sz</span>):</span><br><span class="line">    mask = (torch.triu(torch.ones(sz, sz)) == <span class="number">1</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    mask = mask.<span class="built_in">float</span>().masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)).masked_fill(mask == <span class="number">1</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">input_dim = <span class="number">1000</span>  <span class="comment"># Vocabulary size</span></span><br><span class="line">output_dim = <span class="number">1000</span>  <span class="comment"># Output dimension</span></span><br><span class="line">seq_length = <span class="number">10</span>  <span class="comment"># Sequence length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a model instance</span></span><br><span class="line">model = TransformerModel(input_dim=input_dim, output_dim=output_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example data</span></span><br><span class="line">src = torch.randint(<span class="number">0</span>, input_dim, (seq_length, <span class="number">32</span>))  <span class="comment"># (sequence length, batch size)</span></span><br><span class="line">src_mask = generate_square_subsequent_mask(seq_length)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Forward propagation</span></span><br><span class="line">output = model(src, src_mask)</span><br><span class="line"><span class="built_in">print</span>(output.shape)  <span class="comment"># Expected output: [sequence length, batch size, output dimension]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a simple loss function and optimizer for training</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># Number of iterations</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(src, src_mask)</span><br><span class="line">    loss = criterion(output.view(-<span class="number">1</span>, output_dim), src.view(-<span class="number">1</span>))</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/06/Deep-Dive-into-ReactJS-Hooks/" rel="prev" title="Deep Dive into ReactJS Hooks">
                  <i class="fa fa-angle-left"></i> Deep Dive into ReactJS Hooks
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/12/04/Setting-up-Node-js-development-environment-in-Windows/" rel="next" title="Setting up Node.js development environment in Windows">
                  Setting up Node.js development environment in Windows <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Dazhi Zhang</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.9.1/mermaid.min.js","integrity":"sha256-YbM1pG3wWnzhyYN49g5fPnen+2CKEFaZfopkkwSpNtY="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  





</body>
</html>
